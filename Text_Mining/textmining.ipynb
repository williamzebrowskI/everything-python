{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big-O notation used to describe algorithm performance.\n",
      "\n",
      "This notation format is used to describe how a particular algorithm performs \n",
      "as the size of the set of input grows over time. And the reason the letter O is \n",
      "used is because the growth rate of an algorithm's time complexity is also referred to \n",
      "as the order of operation. \n",
      "\n",
      "This is just a test.  There is no need to panic.  We are here and we are safe.\n",
      "\n",
      "Files in this corpus :  ['notes.txt']\n"
     ]
    }
   ],
   "source": [
    "corpus=PlaintextCorpusReader(os.getcwd(), 'notes.txt')\n",
    "print(corpus.raw())\n",
    "\n",
    "print('Files in this corpus : ', corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/bdtmember/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39m#Extract paragraphs from the corpus\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000002?line=1'>2</a>\u001b[0m paragraphs\u001b[39m=\u001b[39mcorpus\u001b[39m.\u001b[39mparas()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39;49m(corpus\u001b[39m.\u001b[39;49mparas())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/collections.py:226\u001b[0m, in \u001b[0;36mAbstractLazySequence.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m pieces \u001b[39m=\u001b[39m []\n\u001b[1;32m    225\u001b[0m length \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m--> 226\u001b[0m \u001b[39mfor\u001b[39;00m elt \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    227\u001b[0m     pieces\u001b[39m.\u001b[39mappend(\u001b[39mrepr\u001b[39m(elt))\n\u001b[1;32m    228\u001b[0m     length \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pieces[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/reader/util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_toknum \u001b[39m=\u001b[39m toknum\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_blocknum \u001b[39m=\u001b[39m block_index\n\u001b[0;32m--> 306\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream)\n\u001b[1;32m    307\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mblock reader \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m() should return list or tuple.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_block\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m num_toks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokens)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/reader/plaintext.py:137\u001b[0m, in \u001b[0;36mPlaintextCorpusReader._read_para_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    132\u001b[0m paras \u001b[39m=\u001b[39m []\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m para \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_para_block_reader(stream):\n\u001b[1;32m    134\u001b[0m     paras\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    135\u001b[0m         [\n\u001b[1;32m    136\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m--> 137\u001b[0m             \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sent_tokenizer\u001b[39m.\u001b[39;49mtokenize(para)\n\u001b[1;32m    138\u001b[0m         ]\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m paras\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:903\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr):\n\u001b[0;32m--> 903\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    904\u001b[0m     \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:895\u001b[0m, in \u001b[0;36mLazyLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__load\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 895\u001b[0m     resource \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[1;32m    896\u001b[0m     \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[39m# the object by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39m# match that of `resource`.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m resource\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/bdtmember/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Extract paragraphs from the corpus\n",
    "paragraphs=corpus.paras()\n",
    "print(corpus.paras())\n",
    "#print('\\n Total paragraphs in this corpus : ', len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/bdtmember/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m#Extract sentences from the corpus\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000003?line=1'>2</a>\u001b[0m sentences\u001b[39m=\u001b[39mcorpus\u001b[39m.\u001b[39msents()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Total sentences in this corpus : \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39;49m(sentences))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bdtmember/Desktop/Python/Text_Mining/textmining.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m The first sentence : \u001b[39m\u001b[39m'\u001b[39m, sentences[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/reader/util.py:240\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_len \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         \u001b[39m# iterate_from() sets self._len when it reaches the end\u001b[39;00m\n\u001b[1;32m    239\u001b[0m         \u001b[39m# of the file:\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate_from(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_toknum[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[1;32m    241\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_len\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/reader/util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_toknum \u001b[39m=\u001b[39m toknum\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_blocknum \u001b[39m=\u001b[39m block_index\n\u001b[0;32m--> 306\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream)\n\u001b[1;32m    307\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mblock reader \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m() should return list or tuple.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_block\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m num_toks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokens)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/reader/plaintext.py:126\u001b[0m, in \u001b[0;36mPlaintextCorpusReader._read_sent_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    121\u001b[0m sents \u001b[39m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m para \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_para_block_reader(stream):\n\u001b[1;32m    123\u001b[0m     sents\u001b[39m.\u001b[39mextend(\n\u001b[1;32m    124\u001b[0m         [\n\u001b[1;32m    125\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m--> 126\u001b[0m             \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sent_tokenizer\u001b[39m.\u001b[39;49mtokenize(para)\n\u001b[1;32m    127\u001b[0m         ]\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[39mreturn\u001b[39;00m sents\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:903\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr):\n\u001b[0;32m--> 903\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    904\u001b[0m     \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:895\u001b[0m, in \u001b[0;36mLazyLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__load\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 895\u001b[0m     resource \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[1;32m    896\u001b[0m     \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[39m# the object by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39m# match that of `resource`.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m resource\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/bdtmember/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Extract sentences from the corpus\n",
    "sentences=corpus.sents()\n",
    "print('\\n Total sentences in this corpus : ', len(sentences))\n",
    "print('\\n The first sentence : ', sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words in this corpus :  ['Big', '-', 'O', 'notation', 'used', 'to', 'describe', ...]\n"
     ]
    }
   ],
   "source": [
    "#Extract words from the corpus\n",
    "print('\\n Words in this corpus : ',corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the frequency distribution of the wods in the corpus\n",
    "course_freq_dist=nltk.FreqDist(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the corpus :  [('the', 6), ('is', 4), ('of', 4), ('used', 3), ('to', 3), ('algorithm', 3), ('.', 3), ('O', 2), ('notation', 2), ('describe', 2)]\n"
     ]
    }
   ],
   "source": [
    "#Print most commenly used words\n",
    "print('Top 10 words in the corpus : ', course_freq_dist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Distribution for \"is\" :  4\n"
     ]
    }
   ],
   "source": [
    "#find the distrbution for a specific word\n",
    "print('\\n Distribution for \\\"is\\\" : ', course_freq_dist.get('is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
